# RAG

This repo is a simple chatbot with RAG (Retrieval Augmented Generation).
It can answer questions based on information in txt and pdf files as well as weather information from [Open-Meteo](https://open-meteo.com/).

## What is RAG
RAG is short for Retrieval Augmented Generation. It means that the relevant information is retrieved and injected into the prompt before the prompt is given to the AI model. This way the AI model can generate a more accurate answer.
To find out what information is relevant an embedding model is used to generate a vector for the prompt and the documents. The documents with the most similar vectors are then used to generate the answer. 
This vector is a way to represent the meaning of the text in a way that can be compared to other texts, and it works by plotting the texts in a multidimensional space (in this case 1024 dimensions) where texts with similar meanings are close to each other.

## How it works
The content of the files and the keyword "weather" get added to a vector database with their vector generated by an AI model from jinaai (jina-embeddings-v3).
When the user asks something the vector for the prompt is also generated. Then the 3 most relevant documents and keywords get queried by comparing the vectors of the prompt and the documents / keyword. If the keyword "weather" the required data is fetched from the weather api by letting Gemini generate a request URL for the API (to easily translate what the user wants to an api request). The relevant documents and if necessary API response is than injected in the meta prompt and given to roberta-base-squad2 by deepset. The AIs answer is then given to the user.

## How to get started
### 1. Install dependencies
With pip:
```bash
pip install .
```
or with poetry:
```bash
poetry install
```
To install torch follow [this](https://pytorch.org/get-started/locally/). This is necessary because torch can't be easily installed with poetry. 

### 2. Create a .env file (optional)
Create a new file with the name `.env` and write
```.env
GEMINI_API_KEY=<your-api-key>
```
in it.

If you skip this step you will not be able to use the weather api because the local LLM can't generate a valid request URL consistently.

### 3. Set up the vector database
Just run
```bash
docker compose up
```
in the project root.

### 4. Add files
Add files with information the AI should have in the `files` directory in the project root.

### 5. Run the rag.py file
```bash
python rag.py
```
When running it for the first time the information has to be but in the vector db so answer `y` when asked if the information should be indexed.

## Example prompts
These prompts were answered by Gemini instead of the currently used local model.

> What is the company Satellytes?
> 
> Answer: Satellytes is a technology company that is always looking for passionate developers. 

> What options for a career does Satellytes provide?
> 
> Answer: Satellytes is looking for passionate developers who want to take on new challenges and continually develop themselves. 

> What is the secret number?
> 
> Answer: The secret number is 54321. 

> How is the weather in Munich?
> 
> Answer: I can tell you that the weather in Munich is currently 12.7°C with a relative humidity of 83%. 
> 
> And tomorrow?
> 
> Answer: The weather in Munich tomorrow will be 12.0°C with a relative humidity of 95%.
> 
> Will it rain tomorrow?
> 
> Answer: Yes, it will rain tomorrow in Munich. 

> What is Satellytes?
> 
> Answer: Satellytes is a technology company that focuses on creating high-quality software solutions.

## Development process
I started by researching about RAG and how it works. Then I implemented the basic structure of the chatbot. After that I implemented the vector database and the embedding model. Then I implemented a first RAG with the help of [this](https://medium.com/@myscale/how-to-build-a-rag-powered-chatbot-with-google-gemini-and-myscaledb-79c0024cd237) tutorial. After that I cleaned up the code and added support for txt files and the weather api. After I had a working version that used Gemini I changed it to use a local model instead for embedding and text generation.
During the development I used [ChatGPT](https://chatgpt.com) to help me to debug code. I also utilized [GitHub Copilot](https://copilot.github.com/) for auto-completion and prototype methods that I later implemented myself.

## Possible improvements

- Use mean pooling on prompts that require the weather api to generate the embedding instead of using an embedded keyword
- Better LLM / SLM model
- Improve meta prompts
- RAG optimized model and RAG transformer pipeline 
